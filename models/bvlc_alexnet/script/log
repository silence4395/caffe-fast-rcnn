I0909 16:10:18.153491 57261 caffe.cpp:312] Use GPU with device ID 0
I0909 16:10:19.088248 57261 caffe.cpp:316] GPU device name: Graphics Device
I0909 16:10:19.891468 57261 net.cpp:347] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0909 16:10:19.891733 57261 net.cpp:82] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/mllib/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRNRistretto"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
  quantization_param {
    bw_layer_in: 8
    bw_layer_out: 8
    fl_layer_in: -5
    fl_layer_out: -1
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRNRistretto"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
  quantization_param {
    bw_layer_in: 8
    bw_layer_out: 8
    fl_layer_in: -3
    fl_layer_out: -1
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0909 16:10:19.891911 57261 layer_factory.hpp:77] Creating layer data
I0909 16:10:19.892043 57261 db_lmdb.cpp:35] Opened lmdb /mllib/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb
I0909 16:10:19.892083 57261 net.cpp:115] Creating Layer data
I0909 16:10:19.892099 57261 net.cpp:490] data -> data
I0909 16:10:19.892133 57261 net.cpp:490] data -> label
I0909 16:10:19.892160 57261 data_transformer.cpp:25] Loading mean file from: data/ilsvrc12/imagenet_mean.binaryproto
I0909 16:10:19.921771 57261 data_layer.cpp:45] output data size: 50,3,227,227
I0909 16:10:20.016669 57261 net.cpp:153] Setting up data
I0909 16:10:20.016716 57261 net.cpp:160] Top shape: 50 3 227 227 (7729350)
I0909 16:10:20.016726 57261 net.cpp:160] Top shape: 50 (50)
I0909 16:10:20.016731 57261 net.cpp:168] Memory required for data: 30917600
I0909 16:10:20.016744 57261 layer_factory.hpp:77] Creating layer label_data_1_split
I0909 16:10:20.016763 57261 net.cpp:115] Creating Layer label_data_1_split
I0909 16:10:20.016775 57261 net.cpp:516] label_data_1_split <- label
I0909 16:10:20.016811 57261 net.cpp:490] label_data_1_split -> label_data_1_split_0
I0909 16:10:20.016829 57261 net.cpp:490] label_data_1_split -> label_data_1_split_1
I0909 16:10:20.016881 57261 net.cpp:153] Setting up label_data_1_split
I0909 16:10:20.016894 57261 net.cpp:160] Top shape: 50 (50)
I0909 16:10:20.016901 57261 net.cpp:160] Top shape: 50 (50)
I0909 16:10:20.016906 57261 net.cpp:168] Memory required for data: 30918000
I0909 16:10:20.016911 57261 layer_factory.hpp:77] Creating layer conv1
I0909 16:10:20.016937 57261 net.cpp:115] Creating Layer conv1
I0909 16:10:20.016943 57261 net.cpp:516] conv1 <- data
I0909 16:10:20.016952 57261 net.cpp:490] conv1 -> conv1
I0909 16:10:20.024637 57261 net.cpp:153] Setting up conv1
I0909 16:10:20.024667 57261 net.cpp:160] Top shape: 50 96 55 55 (14520000)
I0909 16:10:20.024673 57261 net.cpp:168] Memory required for data: 88998000
I0909 16:10:20.024698 57261 layer_factory.hpp:77] Creating layer relu1
I0909 16:10:20.024713 57261 net.cpp:115] Creating Layer relu1
I0909 16:10:20.024719 57261 net.cpp:516] relu1 <- conv1
I0909 16:10:20.024729 57261 net.cpp:477] relu1 -> conv1 (in-place)
I0909 16:10:20.024740 57261 net.cpp:153] Setting up relu1
I0909 16:10:20.024749 57261 net.cpp:160] Top shape: 50 96 55 55 (14520000)
I0909 16:10:20.024754 57261 net.cpp:168] Memory required for data: 147078000
I0909 16:10:20.024758 57261 layer_factory.hpp:77] Creating layer norm1
I0909 16:10:20.024785 57261 net.cpp:115] Creating Layer norm1
I0909 16:10:20.024811 57261 net.cpp:516] norm1 <- conv1
I0909 16:10:20.024821 57261 net.cpp:490] norm1 -> norm1
I0909 16:10:20.024900 57261 net.cpp:153] Setting up norm1
I0909 16:10:20.024914 57261 net.cpp:160] Top shape: 50 96 55 55 (14520000)
I0909 16:10:20.024920 57261 net.cpp:168] Memory required for data: 205158000
I0909 16:10:20.024925 57261 layer_factory.hpp:77] Creating layer pool1
I0909 16:10:20.024935 57261 net.cpp:115] Creating Layer pool1
I0909 16:10:20.024941 57261 net.cpp:516] pool1 <- norm1
I0909 16:10:20.024950 57261 net.cpp:490] pool1 -> pool1
I0909 16:10:20.025002 57261 net.cpp:153] Setting up pool1
I0909 16:10:20.025013 57261 net.cpp:160] Top shape: 50 96 27 27 (3499200)
I0909 16:10:20.025019 57261 net.cpp:168] Memory required for data: 219154800
I0909 16:10:20.025024 57261 layer_factory.hpp:77] Creating layer conv2
I0909 16:10:20.025038 57261 net.cpp:115] Creating Layer conv2
I0909 16:10:20.025044 57261 net.cpp:516] conv2 <- pool1
I0909 16:10:20.025053 57261 net.cpp:490] conv2 -> conv2
I0909 16:10:20.035822 57261 net.cpp:153] Setting up conv2
I0909 16:10:20.035857 57261 net.cpp:160] Top shape: 50 256 27 27 (9331200)
I0909 16:10:20.035864 57261 net.cpp:168] Memory required for data: 256479600
I0909 16:10:20.035882 57261 layer_factory.hpp:77] Creating layer relu2
I0909 16:10:20.035897 57261 net.cpp:115] Creating Layer relu2
I0909 16:10:20.035905 57261 net.cpp:516] relu2 <- conv2
I0909 16:10:20.035915 57261 net.cpp:477] relu2 -> conv2 (in-place)
I0909 16:10:20.035929 57261 net.cpp:153] Setting up relu2
I0909 16:10:20.035936 57261 net.cpp:160] Top shape: 50 256 27 27 (9331200)
I0909 16:10:20.035941 57261 net.cpp:168] Memory required for data: 293804400
I0909 16:10:20.035946 57261 layer_factory.hpp:77] Creating layer norm2
I0909 16:10:20.035964 57261 net.cpp:115] Creating Layer norm2
I0909 16:10:20.035971 57261 net.cpp:516] norm2 <- conv2
I0909 16:10:20.035980 57261 net.cpp:490] norm2 -> norm2
I0909 16:10:20.036027 57261 net.cpp:153] Setting up norm2
I0909 16:10:20.036041 57261 net.cpp:160] Top shape: 50 256 27 27 (9331200)
I0909 16:10:20.036047 57261 net.cpp:168] Memory required for data: 331129200
I0909 16:10:20.036053 57261 layer_factory.hpp:77] Creating layer pool2
I0909 16:10:20.036065 57261 net.cpp:115] Creating Layer pool2
I0909 16:10:20.036072 57261 net.cpp:516] pool2 <- norm2
I0909 16:10:20.036079 57261 net.cpp:490] pool2 -> pool2
I0909 16:10:20.036119 57261 net.cpp:153] Setting up pool2
I0909 16:10:20.036129 57261 net.cpp:160] Top shape: 50 256 13 13 (2163200)
I0909 16:10:20.036134 57261 net.cpp:168] Memory required for data: 339782000
I0909 16:10:20.036140 57261 layer_factory.hpp:77] Creating layer conv3
I0909 16:10:20.036156 57261 net.cpp:115] Creating Layer conv3
I0909 16:10:20.036164 57261 net.cpp:516] conv3 <- pool2
I0909 16:10:20.036173 57261 net.cpp:490] conv3 -> conv3
I0909 16:10:20.064800 57261 net.cpp:153] Setting up conv3
I0909 16:10:20.064834 57261 net.cpp:160] Top shape: 50 384 13 13 (3244800)
I0909 16:10:20.064841 57261 net.cpp:168] Memory required for data: 352761200
I0909 16:10:20.064857 57261 layer_factory.hpp:77] Creating layer relu3
I0909 16:10:20.064872 57261 net.cpp:115] Creating Layer relu3
I0909 16:10:20.064879 57261 net.cpp:516] relu3 <- conv3
I0909 16:10:20.064889 57261 net.cpp:477] relu3 -> conv3 (in-place)
I0909 16:10:20.064901 57261 net.cpp:153] Setting up relu3
I0909 16:10:20.064908 57261 net.cpp:160] Top shape: 50 384 13 13 (3244800)
I0909 16:10:20.064913 57261 net.cpp:168] Memory required for data: 365740400
I0909 16:10:20.064918 57261 layer_factory.hpp:77] Creating layer conv4
I0909 16:10:20.064934 57261 net.cpp:115] Creating Layer conv4
I0909 16:10:20.064941 57261 net.cpp:516] conv4 <- conv3
I0909 16:10:20.064951 57261 net.cpp:490] conv4 -> conv4
I0909 16:10:20.086233 57261 net.cpp:153] Setting up conv4
I0909 16:10:20.086274 57261 net.cpp:160] Top shape: 50 384 13 13 (3244800)
I0909 16:10:20.086282 57261 net.cpp:168] Memory required for data: 378719600
I0909 16:10:20.086293 57261 layer_factory.hpp:77] Creating layer relu4
I0909 16:10:20.086308 57261 net.cpp:115] Creating Layer relu4
I0909 16:10:20.086318 57261 net.cpp:516] relu4 <- conv4
I0909 16:10:20.086369 57261 net.cpp:477] relu4 -> conv4 (in-place)
I0909 16:10:20.086387 57261 net.cpp:153] Setting up relu4
I0909 16:10:20.086395 57261 net.cpp:160] Top shape: 50 384 13 13 (3244800)
I0909 16:10:20.086400 57261 net.cpp:168] Memory required for data: 391698800
I0909 16:10:20.086405 57261 layer_factory.hpp:77] Creating layer conv5
I0909 16:10:20.086423 57261 net.cpp:115] Creating Layer conv5
I0909 16:10:20.086431 57261 net.cpp:516] conv5 <- conv4
I0909 16:10:20.086439 57261 net.cpp:490] conv5 -> conv5
I0909 16:10:20.104544 57261 net.cpp:153] Setting up conv5
I0909 16:10:20.104578 57261 net.cpp:160] Top shape: 50 256 13 13 (2163200)
I0909 16:10:20.104584 57261 net.cpp:168] Memory required for data: 400351600
I0909 16:10:20.104602 57261 layer_factory.hpp:77] Creating layer relu5
I0909 16:10:20.104617 57261 net.cpp:115] Creating Layer relu5
I0909 16:10:20.104624 57261 net.cpp:516] relu5 <- conv5
I0909 16:10:20.104635 57261 net.cpp:477] relu5 -> conv5 (in-place)
I0909 16:10:20.104648 57261 net.cpp:153] Setting up relu5
I0909 16:10:20.104655 57261 net.cpp:160] Top shape: 50 256 13 13 (2163200)
I0909 16:10:20.104660 57261 net.cpp:168] Memory required for data: 409004400
I0909 16:10:20.104665 57261 layer_factory.hpp:77] Creating layer pool5
I0909 16:10:20.104677 57261 net.cpp:115] Creating Layer pool5
I0909 16:10:20.104686 57261 net.cpp:516] pool5 <- conv5
I0909 16:10:20.104696 57261 net.cpp:490] pool5 -> pool5
I0909 16:10:20.104740 57261 net.cpp:153] Setting up pool5
I0909 16:10:20.104753 57261 net.cpp:160] Top shape: 50 256 6 6 (460800)
I0909 16:10:20.104758 57261 net.cpp:168] Memory required for data: 410847600
I0909 16:10:20.104765 57261 layer_factory.hpp:77] Creating layer fc6
I0909 16:10:20.104785 57261 net.cpp:115] Creating Layer fc6
I0909 16:10:20.104799 57261 net.cpp:516] fc6 <- pool5
I0909 16:10:20.104807 57261 net.cpp:490] fc6 -> fc6
I0909 16:10:21.308933 57261 net.cpp:153] Setting up fc6
I0909 16:10:21.308976 57261 net.cpp:160] Top shape: 50 4096 (204800)
I0909 16:10:21.308984 57261 net.cpp:168] Memory required for data: 411666800
I0909 16:10:21.308997 57261 layer_factory.hpp:77] Creating layer relu6
I0909 16:10:21.309012 57261 net.cpp:115] Creating Layer relu6
I0909 16:10:21.309020 57261 net.cpp:516] relu6 <- fc6
I0909 16:10:21.309031 57261 net.cpp:477] relu6 -> fc6 (in-place)
I0909 16:10:21.309046 57261 net.cpp:153] Setting up relu6
I0909 16:10:21.309052 57261 net.cpp:160] Top shape: 50 4096 (204800)
I0909 16:10:21.309057 57261 net.cpp:168] Memory required for data: 412486000
I0909 16:10:21.309062 57261 layer_factory.hpp:77] Creating layer drop6
I0909 16:10:21.309083 57261 net.cpp:115] Creating Layer drop6
I0909 16:10:21.309092 57261 net.cpp:516] drop6 <- fc6
I0909 16:10:21.309098 57261 net.cpp:477] drop6 -> fc6 (in-place)
I0909 16:10:21.309137 57261 net.cpp:153] Setting up drop6
I0909 16:10:21.309147 57261 net.cpp:160] Top shape: 50 4096 (204800)
I0909 16:10:21.309152 57261 net.cpp:168] Memory required for data: 413305200
I0909 16:10:21.309159 57261 layer_factory.hpp:77] Creating layer fc7
I0909 16:10:21.309170 57261 net.cpp:115] Creating Layer fc7
I0909 16:10:21.309176 57261 net.cpp:516] fc7 <- fc6
I0909 16:10:21.309185 57261 net.cpp:490] fc7 -> fc7
I0909 16:10:21.833539 57261 net.cpp:153] Setting up fc7
I0909 16:10:21.833590 57261 net.cpp:160] Top shape: 50 4096 (204800)
I0909 16:10:21.833596 57261 net.cpp:168] Memory required for data: 414124400
I0909 16:10:21.833611 57261 layer_factory.hpp:77] Creating layer relu7
I0909 16:10:21.833626 57261 net.cpp:115] Creating Layer relu7
I0909 16:10:21.833634 57261 net.cpp:516] relu7 <- fc7
I0909 16:10:21.833647 57261 net.cpp:477] relu7 -> fc7 (in-place)
I0909 16:10:21.833659 57261 net.cpp:153] Setting up relu7
I0909 16:10:21.833667 57261 net.cpp:160] Top shape: 50 4096 (204800)
I0909 16:10:21.833672 57261 net.cpp:168] Memory required for data: 414943600
I0909 16:10:21.833676 57261 layer_factory.hpp:77] Creating layer drop7
I0909 16:10:21.833686 57261 net.cpp:115] Creating Layer drop7
I0909 16:10:21.833694 57261 net.cpp:516] drop7 <- fc7
I0909 16:10:21.833701 57261 net.cpp:477] drop7 -> fc7 (in-place)
I0909 16:10:21.833765 57261 net.cpp:153] Setting up drop7
I0909 16:10:21.833776 57261 net.cpp:160] Top shape: 50 4096 (204800)
I0909 16:10:21.833781 57261 net.cpp:168] Memory required for data: 415762800
I0909 16:10:21.833787 57261 layer_factory.hpp:77] Creating layer fc8
I0909 16:10:21.833801 57261 net.cpp:115] Creating Layer fc8
I0909 16:10:21.833806 57261 net.cpp:516] fc8 <- fc7
I0909 16:10:21.833814 57261 net.cpp:490] fc8 -> fc8
I0909 16:10:21.961891 57261 net.cpp:153] Setting up fc8
I0909 16:10:21.961941 57261 net.cpp:160] Top shape: 50 1000 (50000)
I0909 16:10:21.961948 57261 net.cpp:168] Memory required for data: 415962800
I0909 16:10:21.961962 57261 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0909 16:10:21.961977 57261 net.cpp:115] Creating Layer fc8_fc8_0_split
I0909 16:10:21.961984 57261 net.cpp:516] fc8_fc8_0_split <- fc8
I0909 16:10:21.961997 57261 net.cpp:490] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0909 16:10:21.962010 57261 net.cpp:490] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0909 16:10:21.962051 57261 net.cpp:153] Setting up fc8_fc8_0_split
I0909 16:10:21.962064 57261 net.cpp:160] Top shape: 50 1000 (50000)
I0909 16:10:21.962069 57261 net.cpp:160] Top shape: 50 1000 (50000)
I0909 16:10:21.962074 57261 net.cpp:168] Memory required for data: 416362800
I0909 16:10:21.962080 57261 layer_factory.hpp:77] Creating layer accuracy
I0909 16:10:21.962100 57261 net.cpp:115] Creating Layer accuracy
I0909 16:10:21.962108 57261 net.cpp:516] accuracy <- fc8_fc8_0_split_0
I0909 16:10:21.962116 57261 net.cpp:516] accuracy <- label_data_1_split_0
I0909 16:10:21.962122 57261 net.cpp:490] accuracy -> accuracy
I0909 16:10:21.962138 57261 net.cpp:153] Setting up accuracy
I0909 16:10:21.962146 57261 net.cpp:160] Top shape: (1)
I0909 16:10:21.962151 57261 net.cpp:168] Memory required for data: 416362804
I0909 16:10:21.962155 57261 layer_factory.hpp:77] Creating layer loss
I0909 16:10:21.962167 57261 net.cpp:115] Creating Layer loss
I0909 16:10:21.962175 57261 net.cpp:516] loss <- fc8_fc8_0_split_1
I0909 16:10:21.962182 57261 net.cpp:516] loss <- label_data_1_split_1
I0909 16:10:21.962189 57261 net.cpp:490] loss -> loss
I0909 16:10:21.962211 57261 layer_factory.hpp:77] Creating layer loss
I0909 16:10:21.962381 57261 net.cpp:153] Setting up loss
I0909 16:10:21.962396 57261 net.cpp:160] Top shape: (1)
I0909 16:10:21.962401 57261 net.cpp:163]     with loss weight 1
I0909 16:10:21.962437 57261 net.cpp:168] Memory required for data: 416362808
I0909 16:10:21.962445 57261 net.cpp:203]  [ UPDATE INFO ]conv1 needs update itself weights.
I0909 16:10:21.962450 57261 net.cpp:205]  [ UPDATE INFO ]conv1 needs update itself bias.
I0909 16:10:21.962455 57261 net.cpp:203]  [ UPDATE INFO ]conv2 needs update itself weights.
I0909 16:10:21.962458 57261 net.cpp:205]  [ UPDATE INFO ]conv2 needs update itself bias.
I0909 16:10:21.962463 57261 net.cpp:203]  [ UPDATE INFO ]conv3 needs update itself weights.
I0909 16:10:21.962467 57261 net.cpp:205]  [ UPDATE INFO ]conv3 needs update itself bias.
I0909 16:10:21.962471 57261 net.cpp:203]  [ UPDATE INFO ]conv4 needs update itself weights.
I0909 16:10:21.962476 57261 net.cpp:205]  [ UPDATE INFO ]conv4 needs update itself bias.
I0909 16:10:21.962481 57261 net.cpp:203]  [ UPDATE INFO ]conv5 needs update itself weights.
I0909 16:10:21.962486 57261 net.cpp:205]  [ UPDATE INFO ]conv5 needs update itself bias.
I0909 16:10:21.962489 57261 net.cpp:203]  [ UPDATE INFO ]fc6 needs update itself weights.
I0909 16:10:21.962494 57261 net.cpp:205]  [ UPDATE INFO ]fc6 needs update itself bias.
I0909 16:10:21.962498 57261 net.cpp:203]  [ UPDATE INFO ]fc7 needs update itself weights.
I0909 16:10:21.962502 57261 net.cpp:205]  [ UPDATE INFO ]fc7 needs update itself bias.
I0909 16:10:21.962507 57261 net.cpp:203]  [ UPDATE INFO ]fc8 needs update itself weights.
I0909 16:10:21.962512 57261 net.cpp:205]  [ UPDATE INFO ]fc8 needs update itself bias.
I0909 16:10:21.962517 57261 net.cpp:251]  [ DIFF BP INFO] loss needs backward computation.
I0909 16:10:21.962522 57261 net.cpp:253]  [ DIFF BP INFO] accuracy does not need backward computation.
I0909 16:10:21.962554 57261 net.cpp:251]  [ DIFF BP INFO] fc8_fc8_0_split needs backward computation.
I0909 16:10:21.962563 57261 net.cpp:251]  [ DIFF BP INFO] fc8 needs backward computation.
I0909 16:10:21.962569 57261 net.cpp:251]  [ DIFF BP INFO] drop7 needs backward computation.
I0909 16:10:21.962574 57261 net.cpp:251]  [ DIFF BP INFO] relu7 needs backward computation.
I0909 16:10:21.962579 57261 net.cpp:251]  [ DIFF BP INFO] fc7 needs backward computation.
I0909 16:10:21.962585 57261 net.cpp:251]  [ DIFF BP INFO] drop6 needs backward computation.
I0909 16:10:21.962590 57261 net.cpp:251]  [ DIFF BP INFO] relu6 needs backward computation.
I0909 16:10:21.962595 57261 net.cpp:251]  [ DIFF BP INFO] fc6 needs backward computation.
I0909 16:10:21.962600 57261 net.cpp:251]  [ DIFF BP INFO] pool5 needs backward computation.
I0909 16:10:21.962606 57261 net.cpp:251]  [ DIFF BP INFO] relu5 needs backward computation.
I0909 16:10:21.962611 57261 net.cpp:251]  [ DIFF BP INFO] conv5 needs backward computation.
I0909 16:10:21.962616 57261 net.cpp:251]  [ DIFF BP INFO] relu4 needs backward computation.
I0909 16:10:21.962621 57261 net.cpp:251]  [ DIFF BP INFO] conv4 needs backward computation.
I0909 16:10:21.962627 57261 net.cpp:251]  [ DIFF BP INFO] relu3 needs backward computation.
I0909 16:10:21.962632 57261 net.cpp:251]  [ DIFF BP INFO] conv3 needs backward computation.
I0909 16:10:21.962638 57261 net.cpp:251]  [ DIFF BP INFO] pool2 needs backward computation.
I0909 16:10:21.962643 57261 net.cpp:251]  [ DIFF BP INFO] norm2 needs backward computation.
I0909 16:10:21.962648 57261 net.cpp:251]  [ DIFF BP INFO] relu2 needs backward computation.
I0909 16:10:21.962654 57261 net.cpp:251]  [ DIFF BP INFO] conv2 needs backward computation.
I0909 16:10:21.962661 57261 net.cpp:251]  [ DIFF BP INFO] pool1 needs backward computation.
I0909 16:10:21.962667 57261 net.cpp:251]  [ DIFF BP INFO] norm1 needs backward computation.
I0909 16:10:21.962672 57261 net.cpp:251]  [ DIFF BP INFO] relu1 needs backward computation.
I0909 16:10:21.962677 57261 net.cpp:251]  [ DIFF BP INFO] conv1 needs backward computation.
I0909 16:10:21.962683 57261 net.cpp:253]  [ DIFF BP INFO] label_data_1_split does not need backward computation.
I0909 16:10:21.962689 57261 net.cpp:253]  [ DIFF BP INFO] data does not need backward computation.
I0909 16:10:21.962694 57261 net.cpp:295] This network produces output accuracy
I0909 16:10:21.962699 57261 net.cpp:295] This network produces output loss
I0909 16:10:21.962718 57261 net.cpp:308] Network initialization done.
I0909 16:10:22.469580 57261 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: models/bvlc_alexnet/bvlc_alexnet.caffemodel
I0909 16:10:22.469645 57261 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W0909 16:10:22.469655 57261 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0909 16:10:22.469811 57261 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: models/bvlc_alexnet/bvlc_alexnet.caffemodel
I0909 16:10:25.196761 57261 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I0909 16:10:25.264427 57261 caffe.cpp:327] Running for 50 iterations.
I0909 16:10:25.366379 57261 caffe.cpp:350] Batch 0, accuracy = 0.4
I0909 16:10:25.366410 57261 caffe.cpp:350] Batch 0, loss = 2.37638
I0909 16:10:25.417620 57261 caffe.cpp:350] Batch 1, accuracy = 0.64
I0909 16:10:25.417659 57261 caffe.cpp:350] Batch 1, loss = 1.41701
I0909 16:10:25.468729 57261 caffe.cpp:350] Batch 2, accuracy = 0.66
I0909 16:10:25.468776 57261 caffe.cpp:350] Batch 2, loss = 1.11323
I0909 16:10:25.518302 57261 caffe.cpp:350] Batch 3, accuracy = 0.46
I0909 16:10:25.518348 57261 caffe.cpp:350] Batch 3, loss = 2.16279
I0909 16:10:25.566550 57261 caffe.cpp:350] Batch 4, accuracy = 0.52
I0909 16:10:25.566596 57261 caffe.cpp:350] Batch 4, loss = 2.42563
I0909 16:10:25.614770 57261 caffe.cpp:350] Batch 5, accuracy = 0.5
I0909 16:10:25.614817 57261 caffe.cpp:350] Batch 5, loss = 1.50444
I0909 16:10:25.662799 57261 caffe.cpp:350] Batch 6, accuracy = 0.52
I0909 16:10:25.662847 57261 caffe.cpp:350] Batch 6, loss = 2.17923
I0909 16:10:25.708777 57261 caffe.cpp:350] Batch 7, accuracy = 0.56
I0909 16:10:25.708848 57261 caffe.cpp:350] Batch 7, loss = 1.98107
I0909 16:10:25.754801 57261 caffe.cpp:350] Batch 8, accuracy = 0.6
I0909 16:10:25.754848 57261 caffe.cpp:350] Batch 8, loss = 1.48359
I0909 16:10:25.800668 57261 caffe.cpp:350] Batch 9, accuracy = 0.58
I0909 16:10:25.800716 57261 caffe.cpp:350] Batch 9, loss = 2.10029
I0909 16:10:25.846119 57261 caffe.cpp:350] Batch 10, accuracy = 0.54
I0909 16:10:25.846163 57261 caffe.cpp:350] Batch 10, loss = 2.04137
I0909 16:10:25.891077 57261 caffe.cpp:350] Batch 11, accuracy = 0.48
I0909 16:10:25.891122 57261 caffe.cpp:350] Batch 11, loss = 2.13712
I0909 16:10:25.935974 57261 caffe.cpp:350] Batch 12, accuracy = 0.5
I0909 16:10:25.936018 57261 caffe.cpp:350] Batch 12, loss = 2.2141
I0909 16:10:25.980782 57261 caffe.cpp:350] Batch 13, accuracy = 0.6
I0909 16:10:25.980876 57261 caffe.cpp:350] Batch 13, loss = 1.83698
I0909 16:10:26.025630 57261 caffe.cpp:350] Batch 14, accuracy = 0.66
I0909 16:10:26.025677 57261 caffe.cpp:350] Batch 14, loss = 1.38378
I0909 16:10:26.070567 57261 caffe.cpp:350] Batch 15, accuracy = 0.68
I0909 16:10:26.070611 57261 caffe.cpp:350] Batch 15, loss = 1.43794
I0909 16:10:26.115643 57261 caffe.cpp:350] Batch 16, accuracy = 0.6
I0909 16:10:26.115690 57261 caffe.cpp:350] Batch 16, loss = 2.07839
I0909 16:10:26.160696 57261 caffe.cpp:350] Batch 17, accuracy = 0.44
I0909 16:10:26.160742 57261 caffe.cpp:350] Batch 17, loss = 2.07128
I0909 16:10:26.205665 57261 caffe.cpp:350] Batch 18, accuracy = 0.46
I0909 16:10:26.205716 57261 caffe.cpp:350] Batch 18, loss = 2.0072
I0909 16:10:26.250633 57261 caffe.cpp:350] Batch 19, accuracy = 0.5
I0909 16:10:26.250680 57261 caffe.cpp:350] Batch 19, loss = 2.13096
I0909 16:10:26.295624 57261 caffe.cpp:350] Batch 20, accuracy = 0.66
I0909 16:10:26.295670 57261 caffe.cpp:350] Batch 20, loss = 1.65778
I0909 16:10:26.340577 57261 caffe.cpp:350] Batch 21, accuracy = 0.54
I0909 16:10:26.340626 57261 caffe.cpp:350] Batch 21, loss = 2.13201
I0909 16:10:26.385637 57261 caffe.cpp:350] Batch 22, accuracy = 0.52
I0909 16:10:26.385685 57261 caffe.cpp:350] Batch 22, loss = 1.79468
I0909 16:10:26.430743 57261 caffe.cpp:350] Batch 23, accuracy = 0.74
I0909 16:10:26.430789 57261 caffe.cpp:350] Batch 23, loss = 0.943851
I0909 16:10:26.475728 57261 caffe.cpp:350] Batch 24, accuracy = 0.58
I0909 16:10:26.475776 57261 caffe.cpp:350] Batch 24, loss = 1.63776
I0909 16:10:26.520619 57261 caffe.cpp:350] Batch 25, accuracy = 0.52
I0909 16:10:26.520666 57261 caffe.cpp:350] Batch 25, loss = 1.85709
I0909 16:10:26.565620 57261 caffe.cpp:350] Batch 26, accuracy = 0.58
I0909 16:10:26.565667 57261 caffe.cpp:350] Batch 26, loss = 2.18738
I0909 16:10:26.610580 57261 caffe.cpp:350] Batch 27, accuracy = 0.64
I0909 16:10:26.610627 57261 caffe.cpp:350] Batch 27, loss = 1.73058
I0909 16:10:26.655498 57261 caffe.cpp:350] Batch 28, accuracy = 0.52
I0909 16:10:26.655546 57261 caffe.cpp:350] Batch 28, loss = 1.78884
I0909 16:10:26.700358 57261 caffe.cpp:350] Batch 29, accuracy = 0.6
I0909 16:10:26.700403 57261 caffe.cpp:350] Batch 29, loss = 2.01517
I0909 16:10:26.745384 57261 caffe.cpp:350] Batch 30, accuracy = 0.62
I0909 16:10:26.745432 57261 caffe.cpp:350] Batch 30, loss = 1.66051
I0909 16:10:26.790377 57261 caffe.cpp:350] Batch 31, accuracy = 0.64
I0909 16:10:26.790428 57261 caffe.cpp:350] Batch 31, loss = 1.7808
I0909 16:10:26.835301 57261 caffe.cpp:350] Batch 32, accuracy = 0.42
I0909 16:10:26.835348 57261 caffe.cpp:350] Batch 32, loss = 2.49048
I0909 16:10:26.880241 57261 caffe.cpp:350] Batch 33, accuracy = 0.58
I0909 16:10:26.880287 57261 caffe.cpp:350] Batch 33, loss = 1.66003
I0909 16:10:26.925292 57261 caffe.cpp:350] Batch 34, accuracy = 0.64
I0909 16:10:26.925339 57261 caffe.cpp:350] Batch 34, loss = 1.65745
I0909 16:10:26.970408 57261 caffe.cpp:350] Batch 35, accuracy = 0.6
I0909 16:10:26.970456 57261 caffe.cpp:350] Batch 35, loss = 1.61085
I0909 16:10:27.015394 57261 caffe.cpp:350] Batch 36, accuracy = 0.64
I0909 16:10:27.015439 57261 caffe.cpp:350] Batch 36, loss = 1.65251
I0909 16:10:27.060330 57261 caffe.cpp:350] Batch 37, accuracy = 0.56
I0909 16:10:27.060375 57261 caffe.cpp:350] Batch 37, loss = 1.82244
I0909 16:10:27.105381 57261 caffe.cpp:350] Batch 38, accuracy = 0.42
I0909 16:10:27.105430 57261 caffe.cpp:350] Batch 38, loss = 2.0044
I0909 16:10:27.150285 57261 caffe.cpp:350] Batch 39, accuracy = 0.46
I0909 16:10:27.150334 57261 caffe.cpp:350] Batch 39, loss = 2.33714
I0909 16:10:27.195354 57261 caffe.cpp:350] Batch 40, accuracy = 0.52
I0909 16:10:27.195399 57261 caffe.cpp:350] Batch 40, loss = 2.2835
I0909 16:10:27.240320 57261 caffe.cpp:350] Batch 41, accuracy = 0.52
I0909 16:10:27.240368 57261 caffe.cpp:350] Batch 41, loss = 1.72226
I0909 16:10:27.285240 57261 caffe.cpp:350] Batch 42, accuracy = 0.58
I0909 16:10:27.285289 57261 caffe.cpp:350] Batch 42, loss = 1.61499
I0909 16:10:27.330302 57261 caffe.cpp:350] Batch 43, accuracy = 0.54
I0909 16:10:27.330349 57261 caffe.cpp:350] Batch 43, loss = 1.67707
I0909 16:10:27.375253 57261 caffe.cpp:350] Batch 44, accuracy = 0.56
I0909 16:10:27.375300 57261 caffe.cpp:350] Batch 44, loss = 1.50157
I0909 16:10:27.420249 57261 caffe.cpp:350] Batch 45, accuracy = 0.58
I0909 16:10:27.420297 57261 caffe.cpp:350] Batch 45, loss = 1.96463
I0909 16:10:27.465206 57261 caffe.cpp:350] Batch 46, accuracy = 0.74
I0909 16:10:27.465253 57261 caffe.cpp:350] Batch 46, loss = 1.33498
I0909 16:10:27.510141 57261 caffe.cpp:350] Batch 47, accuracy = 0.54
I0909 16:10:27.510188 57261 caffe.cpp:350] Batch 47, loss = 1.89678
I0909 16:10:27.555097 57261 caffe.cpp:350] Batch 48, accuracy = 0.54
I0909 16:10:27.555143 57261 caffe.cpp:350] Batch 48, loss = 1.90777
I0909 16:10:27.600016 57261 caffe.cpp:350] Batch 49, accuracy = 0.64
I0909 16:10:27.600064 57261 caffe.cpp:350] Batch 49, loss = 1.42736
I0909 16:10:27.600071 57261 caffe.cpp:355] Loss: 1.83671
I0909 16:10:27.600096 57261 caffe.cpp:367] accuracy = 0.5628
I0909 16:10:27.600113 57261 caffe.cpp:367] loss = 1.83671 (* 1 = 1.83671 loss)
